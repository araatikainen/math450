{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5.\n",
    "Group name: TAU03E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.\n",
    "The projection of a vector $f$ onto the subspace $M$ is the closest point in $M$ to $f$ in terms of Euclidean distance. This projection $P_M f$ can be represented as a linear combination of the basis vectors $\\left\\{v_1{,}v_2{,}\\dots{,}v_r\\right\\}$ for $M$:\n",
    "\n",
    "$P_Mf=\\sum_{k=1}^r\\alpha_kv_k{,}$, where $\\alpha_k$ is a coefficient.\n",
    "\n",
    "As $v_k$ are orthonormal vectors, therefore $v_i^Tv_j=\\begin{cases}\n",
    "1{,}&\\text{when}\\ i=j\\\\\n",
    "0{,}&\\text{when}\\ i\\ne j\n",
    "\\end{cases}$. To find the coeffiecient, let's take projection with $v_j$ for $j=1{,}\\dots{,}r$.\n",
    "\n",
    "$v_j^TP_Mf=v_j^T\\left(\\sum_{k=1}^r\\alpha_kv_k\\right)\\Rightarrow v_j^TP_Mf=\\sum_{k=1}^r\\alpha_kv_j^Tv_k=\\alpha_j$.\n",
    "\n",
    "This means that $\\alpha_j=v_j^Tf=f^Tv_j$ and hence, it can be written that $P_Mf=\\sum _{k=1}^r\\left(f^Tv_k\\right)v_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.\n",
    "\n",
    "The $P_{My}$ is orthogonal projection of $y$ into $M$. From the squared distance $||y-x||^2$, the $x=P_{My}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The $P_{My}$ is orthogonal in $M$ and $y-P_{My}\\perp p \\in M$, where $p \\in M$ is every vector in $M$.\n",
    "Thus, the \n",
    "\n",
    "$\n",
    "||y-x||^2 = ||y-P_{My} + P_{My}-x||^2\n",
    "$,\n",
    "\n",
    "where $y-P_{My} \\perp P_{My}-x$. Therefore\n",
    "\n",
    "$ |y-x||^2 = ||y-P_{My} ||^2 + ||P_{My}-x||^2$. \n",
    "\n",
    "Since the $||P_{My}-x||^2$ is squared norm and $x\\in M, P_{My} \\in M$, the squared norm $||P_{My}-x||^2 \\geq 0$ and the minimized squared distance $||y-x||^2 = ||y-P_{My} ||^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.\n",
    "Let's suppose it's the case for vector $x$ that $x\\in N\\left(A\\right)$. Then by definition $Ax=0$.\n",
    "\n",
    "Now it can be shown that $x\\in N\\left(A^TA\\right)$ as $Ax=0$ thus $A^TAx=A^T0=0$. Therefore, $N(A)\\subseteq N(A^TA)$.\n",
    "\n",
    "Let's suppose that $x\\in N\\left(A^TA\\right)$. Then by definition $A^TAx=0$.\n",
    "\n",
    "Now let's consider the inner product $x^T\\left(A^TA\\right)x=\\left(Ax\\right)^T\\left(Ax\\right)=\\left|\\left|Ax\\right|\\right|^2=0$. Since, $\\left|\\left|Ax\\right|\\right|^2=0$ thus $Ax=0$ and hence  $N(A^TA)\\subseteq N(A)$.\n",
    "\n",
    "Now it has been shown that $N(A)\\subseteq N(A^TA)$ and $N(A^TA)\\subseteq N(A)$. Therefore, $N(A)=N(A^TA)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.\n",
    "\n",
    "$G$ is positive definite if $x^\\top G x > 0 $.\n",
    "\n",
    "Now the $x^\\top G x = x^\\top (A^\\top A + L^\\top L)x= x^\\top A^\\top Ax + x^\\top L^\\top Lx = (Ax)^\\top (Ax) + (Lx)^\\top (Lx) = ||Ax||^2 + ||Lx||^2 \\geq 0$.\n",
    "\n",
    "This shows that $G$ is positive semi definite. To shows positive definite, the $x^\\top G x =0$ is only when $x=0$.\n",
    "\n",
    "The $x^\\top G x = 0$, only when\n",
    "\n",
    "$||Ax||^2 = 0$ and $||Lx||^2 = 0$, thus $\\mathcal{N}(A)$ and $\\mathcal{N}(L)$.\n",
    "\n",
    "According to definition, $\\mathcal{N}(A) \\cap \\mathcal{N}(L) = {0}$, the $x=0$. Which means that $G$ is positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously presented, for the $\\mathcal{N}(G) = 0$, the definition states that $Gx = 0$.\n",
    "However, according to $G$ being positive definite and the $x^\\top Gx = 0$ is true only when $x=0$. Therefore, $\\mathcal{N}(G) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.\n",
    "\n",
    "To express\n",
    "$F(x) = || y - A x ||^2 + || R(x - \\mu) ||^2$\n",
    "\n",
    "as matrix form, lets expand both $|| y - A x ||^2$ and $|| R(x - \\mu) ||^2$ separately.\n",
    "\n",
    "By the definition of the norm, \n",
    "\n",
    "$|| y - A x ||^2 = (y - A x)^\\top (y - A x)$\n",
    "\n",
    "$|| R(x - \\mu) ||^2 = (R(x - \\mu))^\\top (R(x - \\mu)) = (x - \\mu)R^\\top R(x - \\mu)$.\n",
    "\n",
    "Now using vectors $(y - A x)$ and $R(x - \\mu)$ represent the $F(x)= |y - A x |^2 + |R(x - \\mu)|^2$.\n",
    "\n",
    "In single norm expression $F$ becomes\n",
    "\n",
    "$|(y - (-R\\mu)) - (Ax - Rx)|^2 = |(y + R\\mu) - (A- R)x|^2$,\n",
    "\n",
    "which can be used to represent original $F$ in matrix form as\n",
    "\n",
    "$F(x) = \\left\\| \\begin{pmatrix} y \\\\ -R \\mu \\end{pmatrix} - \\begin{pmatrix} A \\\\ -R \\end{pmatrix} x \\right\\|^2.$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
