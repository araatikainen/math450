{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.\n",
    "Group name: TAU03E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Random variables are defined as thigns that get different values each time observed.\n",
    "\n",
    "a) No. Equation is not random variable since equation has certain value for $x$ where it is true. There is no randomness in solution\n",
    "\n",
    "b) Yes. Security camera picture is probably different each time it is taken. Thus, it can be a random variable\n",
    "\n",
    "c) Yes. Daily stock price varies on multiple factors and variables.\n",
    "\n",
    "d) No. All the variables are known for the mass index $BMI$. Thus, it cannot be a random variable.\n",
    "\n",
    "e) Yes. One variable now is unknown for the mass index $BMI$, which leads to the fact it can be modeled with probability distribution and result is dependent on the unknown variable.\n",
    "\n",
    "f) No. Distribution of students is not a random variable.\n",
    "\n",
    "e) No. Physics theorem validity is not a random variable since it is objective.\n",
    "\n",
    "g) No. Mathematical hypothesis are also objective matters. However they can be studied and tested using random variables but the hypothesis validity is not a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "### a)\n",
    "Let's show that $\\hat{\\mu}=\\mu$ as $E\\left(\\hat{\\mu}\\right)=E\\left(\\frac{1}{N}\\sum_{i=1}^Nx_i\\right)\\Rightarrow\\ \\hat{\\mu}=\\frac{1}{N}\\sum_{i=1}^NE\\left(x_i\\right)$. Now as $E\\left(x_i\\right)=\\mu$ therefore $\\hat{\\mu }=\\frac{1}{N}\\sum _{i=1}^N\\mu =\\frac{N\\mu }{N}=\\mu $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "(We assume that that the equation is missing a minus sign by accident in the assignment).\n",
    "\n",
    "Let's show that $\\hat{\\sigma^2}\\neq\\sigma^2$ as $\\hat{\\sigma^2}=\\frac{1}{N}\\sum_{i=1}^N\\left(X_i-\\hat{\\mu}\\right)^2=\\frac{1}{N}\\sum_{i=1}^N\\left(X_i^2-2X_i\\hat{\\mu}+\\hat{\\mu^2}\\right)=\\frac{1}{N}\\left(\\sum_{i=1}^NX_i^2-2\\hat{\\mu}\\sum_{i=1}^NX_i+n\\hat{\\mu^2}\\right)$.\n",
    "\n",
    "Now it's known that $\\sum_{i=1}^NX_i=n\\cdot\\frac{1}{n}\\sum_{i=1}^NX_i=n\\hat{\\mu}$. Thus $\\hat{\\sigma^2}=\\frac{1}{N}\\left(\\sum_{i=1}^NX_i^2-2n\\hat{\\mu^2}+n\\hat{\\mu^2}\\right)=\\frac{1}{N}\\left(\\sum_{i=1}^NX_i^2-n\\hat{\\mu^2}\\right)$.\n",
    "\n",
    "Let's take the expectation of the estimator to show that it's biased:\n",
    "\n",
    " $E\\left(\\hat{\\sigma^2}\\right)=E\\left(\\frac{1}{N}\\left(\\sum_{i=1}^NX_i^2-n\\hat{\\mu^2}\\right)\\right)=\\frac{1}{N}\\left(\\sum_{i=1}^NE\\left(X_i^2\\right)-nE\\left(\\hat{\\mu^2}\\right)\\right)=E\\left(X_i^2\\right)-E\\left(\\hat{\\mu^2}\\right)$.\n",
    "\n",
    "Now as $Var\\left(X\\right)=E\\left(X^2\\right)-E\\left(X\\right)^2\\Rightarrow\\ \\sigma^2=E\\left(X^2\\right)-\\mu^2\\Rightarrow E\\left(X^2\\right)=\\sigma^2+\\mu^2$.\n",
    "\n",
    "Also $Var\\left(\\hat{\\mu}\\right)=Var\\left(\\frac{1}{N}\\sum_{i=1}^Nx_i\\right)=\\frac{\\sigma^2}{n}$ and thus, $Var\\left(\\hat{\\mu }\\right)=E\\left(\\hat{\\mu }^2\\right)-E\\left(\\hat{\\mu }\\right)^2\\Rightarrow \\ E\\left(\\hat{\\mu }^2\\right)=\\frac{\\sigma ^2}{n}+\\mu ^2$.\n",
    "\n",
    "Hence, $E\\left(X_i^2\\right)-E\\left(\\hat{\\mu^2}\\right)=\\sigma^2+\\mu^2-\\frac{\\sigma^2}{n}-\\mu^2=\\left(1-\\frac{1}{n}\\right)\\sigma^2\\ne\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "Now that it's known that $E\\left(\\hat{\\sigma^2}\\right)=\\left(1-\\frac{1}{n}\\right)\\sigma^2=\\frac{n-1}{n}\\sigma^2$. To get the unbiased estimator, we just have to multiply the $E\\left(\\hat{\\sigma^2}\\right)$ by $\\frac{n}{n-1}$ \n",
    "\n",
    "as $\\frac{n}{n-1}E\\left(\\hat{\\sigma^2}\\right)=\\left(\\frac{n}{n-1}\\right)\\left(\\frac{n-1}{n}\\right)\\sigma^2=\\sigma^2$.\n",
    "\n",
    "Therefore, the unbiased estimator is $\\left(\\frac{N}{N-1}\\right)\\sigma^2=\\left(\\frac{N}{N-1}\\right)\\left(\\frac{1}{N}\\right)\\sum_{i=1}^N\\left(X_i-\\hat{\\mu}\\right)^2=\\left(\\frac{1}{N-1}\\right)\\sum_{i=1}^N\\left(X_i-\\hat{\\mu}\\right)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional distribution of a multivariate normal distribution with two dimensions is just a normal distribution.\n",
    "\n",
    "We can calculate the mean of the conditional distribution $\\mu_{C} = \\mu_2 + \\Sigma_{2,1}\\Sigma^{-1}_{1}(x_1 - \\mu_1)$, where $\\mu_2$ is the mean of the second random variable and $\\Sigma_{2,1}$ the covariate matrix of both variables, $\\Sigma^{-1}_{1}$ the inverse of the variance of the first random variable and $\\mu_1$ the mean of the first variable.\n",
    "\n",
    "By inserting the values we get\n",
    "\n",
    "$$\n",
    "\n",
    "\\mu_C = 8 + 3 \\cdot 4^{-1} \\cdot (10 - 4) \\\\\n",
    "\\mu_C = 12.5\n",
    "\n",
    "$$\n",
    "\n",
    "as the mean of the conditional distribution. \n",
    "\n",
    "The conditional variance is calculated with $\\Sigma_C = \\Sigma_2 - \\Sigma_{2,1} \\Sigma_1^{-1} \\Sigma_{2,1}^T$. Let's calculate.\n",
    "\n",
    "$$\n",
    "\n",
    "\\Sigma_C = 4 - 3 \\cdot 4^{-1} \\cdot 3 \\\\\n",
    "\\Sigma_C = 1.75\n",
    "\n",
    "$$\n",
    "\n",
    "Thus, we get  $\\mathcal{N}\\sim(12.5, 1.75)$ as the conditional distribution $p(x_2 | x_1 = 10)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the variance of the distribution $p(x_2 | x_1)$ is defined as $\\Sigma_C = \\Sigma_2 - \\Sigma_{2,1} \\Sigma_1^{-1} \\Sigma_{2,1}^T$, where $\\Sigma_2$ stands for the variance of $X_2$, $\\Sigma_{2,1}$ the covariance between $X_1$ and $X_2$ and $\\Sigma_1$ the variance of $X_1$.\n",
    "\n",
    "In terms of statistical coefficients, variance and covariance are defined based on all possible values of $X_1$ and $X_2$, i.e. all possible observations. Now, because none of the terms of $\\Sigma_C$ rely solely on a single observation of the random variable $X_1$, we can say that the observed value of $x_1$ in the conditional variable does not affect the variance of the conditional distribution.\n",
    "\n",
    "In case of $p(x_2 | x_1 = 1)$, $p(x_2 | x_1 = 2)$ and $p(x_2 | x_1 = 3)$, we also know that the covariance matrix is the same for all values of $x_1$ and $x_2$. Thus, $\\Sigma_C$ does not change with different observations of $x_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "### a)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
